%% ****** Start of file apstemplate.tex ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4 distribution.
%%   Version 4.1r of REVTeX, August 2010
%%
%%
%%   Copyright (c) 2001, 2009, 2010 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, prstper, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%  Add 'showkeys' option to make keywords appear
%\documentclass[aps,prl,preprint,groupedaddress]{revtex4-1}
%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-1}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-1}
\documentclass[11pt, oneside, a4paper]{article}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-1}

\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{A Parallel Geometric Multigrid Algorithm}

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Oliver Henrich$^1$, Kevin Stratford$^1$, S\'ebastien Loisel$^2$\\
$^1$ EPCC, School of Physics and Astronomy,\\
University of Edinburgh, Edinburgh EH9 3JZ, UK\\
$^2$ Department of Mathematics,\\ 
Herriot-Watt University, Edinburgh EH14 4AS \\
}
%\email[]{Your e-mail address}
%\homepage[]{Your web page}
%\thanks{}
%\altaffiliation{}
%\affiliation{}
\date{\today}
%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation


% insert suggested PACS numbers in braces on next line
%\pacs{}
% insert suggested keywords - APS authors don't need to do this
%\keywords{}
%\begin{abstract}
%Blablabla
%\end{abstract}

%\maketitle must follow title, authors, abstract, \pacs, and \keywords
\maketitle

% body of paper here - Use proper section commands
% References should be done using the \cite, \ref, and \label commands
\section{Definitions}

\begin{eqnarray*}
k&\dots&\mbox{grid level $1\le k \le k_{max}$ with $k=1$ being the coarsest grid}\\
\Omega_k &\dots& \mbox{regular grid with spacing $h(k)$}\\
\partial \Omega_k &\dots& \mbox{boundary of computational domain}\\
m&\dots& \mbox{iteration index}\\
u_k&\dots& \mbox{exact solution on $\Omega_k$}\\ 
v_k^m&\dots& \mbox{approximate solution on $\Omega_k$ after $m$ iterations}\\
e_k^m&\dots& \mbox{error on $\Omega_k$ after $m$ iterations}\\ 
R_h^H, R_k^{k-1}&\dots& \mbox{restriction operator: mapping functions on grid level $k$ with} \\
&& \mbox{spacing $h$ to functions on the next coarser level $k-1$ with spacing $H=2h$}\\ 
I_H^h, I_{k-1}^k&\dots& \mbox{interpolation operator: mapping functions on grid level $k-1$ with }\\
&&\mbox{spacing $H=2h$ to functions on the next finer level $k$ with spacing $h$}\\ 
\end{eqnarray*}


\section{Transfer Operators}

\subsection{Restriction}

We use half weighting (HW) as this restriction operator has reduced communication requirements. In stencil notation the effect of this operator is

\begin{eqnarray*}
f_H(x,y,z) &=& R_h^H f_h(x,y,z) = \frac{1}{12}[ f_h(x-h,y,z)+f_h(x+h,y,z)+f_h(x,y-h,z)\\
&& +f_h(x,y+h,z)+f_h(x,y,z-h)+f_h(x,y,z+h)+6\;f_h(x,y,z)].
\end{eqnarray*}

The operations are independent of each other and can be performed in parallel. The computation can also be overlapped with communication of the halo data.

\subsection{Interpolation}

We use trilinear interpolation. Note that the coordinate indices $x,y$ and $z$ refer to the indices on the fine grid $\Omega_h$.

\begin{equation*}
f_h(x,y,z) = I_H^h f_H(x,y,z)
\end{equation*}

\begin{equation*}
f_h(x,y,z) =\\
\begin{cases}
\;\;\;f_H(x,y,z)& , \mbox{ case } 1\\
\frac{1}{2} [f_H(x,y-h,z)+f_H(x,y+h,z)] &,  \mbox{ case } 2\\
\frac{1}{2} [f_H(x-h,y,z)+f_H(x+h,y,z)] &,  \mbox{ case } 3\\
\frac{1}{4} [f_H(x-h,y-h,z)+f_H(x+h,y-h,z) \\
\;\;\;+f_H(x-h,y+h,z)+f_H(x+h,y+h,z)] &,  \mbox{ case } 4\\
\frac{1}{8} [f_H(x-h,y-h,z-h)+f_H(x+h,y-h,z-h)\\
\;\;\;+f_H(x-h,y+h,z-h)+f_H(x+h,y+h,z-h)\\
\;\;\;+f_H(x-h,y-h,z+h)+f_H(x+h,y-h,z+h)\\
\;\;\;+f_H(x-h,y+h,z+h)+f_H(x+h,y+h,z+h) &, \mbox{ case } 5
\end{cases}
\vspace*{0.5cm}
\end{equation*}

We distinguish the following cases:

\begin{enumerate}
\item points with $x,y,z\in\Omega_H$ 
\item points with $x,z\in \Omega_H$, $y\notin \Omega_H$  
\item points with $y,z\in \Omega_H$, $x\notin \Omega_H$
\item points with $z\in \Omega_H$, $x,y\notin \Omega_H$ 
\item points with $x,y,z\notin \Omega_H$. 
\end{enumerate}

Also these operations are independent of each other and can be performed in parallel. However, it might not be possible to overlap computation and communication.

\section{Relaxation}

The equation we like to solve is the Poisson Equation $A u = f$ with $A=L=-\Delta$. 
We use the standard 7-point discretisation for the Laplacian:

\begin{eqnarray*}
A_h u_h(x,z,z) &=& \frac{1}{h^2} [6 u_h(x,y,z) - u_h(x+h,y,z) - u_h(x-h,y,z)-u_h(x, y+h,z) \\
 &-& u_h(x,y-h,z) - u_h(x,y,z+h) - u_h(x,y,z-h)] = f_h(x,y,z).
\end{eqnarray*}
 
We use a red-black Gauss-Seidel with overrelaxation parameter. An overrelaxtion parameter of $\omega=1.15$ was reported to lead to an optimal convergence rate. The iteration scheme for the approximation after $m+1$ iterations on grid $\Omega_h$ is

\begin{eqnarray*}
v^{m+1}_h(x,y,z) &=& \frac{1}{6} [h^2\;f_h(x,y,z)\\
&&+\;u_h^m(x+h,y,z)+u_h^m(x-h,y,z)\\
&&+\; u_h^m(x,y+h,z)+u_h^m(x,y-h,z)\\
&&+\; u_h^m(x,y,z+h)+u_h^m(x,y,z-h)]\\
u^{m+1}_h(x,y,z) &\leftarrow& u^m_h(x,y,z)+\omega[v^{m+1}_h(x,y,z)-u^m_h(x,y,z)]
\end{eqnarray*}

where the arrow indicates straight overwriting of the last approximation $u^m_h$ with the new one $u^{m+1}_h$ as it becomes available. All 'red' components on the lefthand side are independent of the 'black' components on the righthand side and can be updated in parallel.

In order overlap computation and communication we relax first the grid points at the boundaries $\partial\Omega_h$ followed by halo swaps via non-blocking MPI calls and compute then the inner grid points $\Omega_h\setminus\partial\Omega_h$.

\section{Coarse-grid communication and solution strategies}

Several coarse-grid communication strategies are suggested in the literature which are supposed to alleviate the communication overhead that emerges when the decreasing ratio of grid points and halo sites starts to have a deteriorating effect on the algorithm. 

Probably the easiest way to deal with this issue is to truncate the multigrid hierarchy so that there are always plenty of grid points on each process. This can have a detrimental effect on the overall-convergence of the algorithm as the lowest grid level is probably still too fine to achieve rapid convergence. It is also possible to use a fast, parallel iterative solver like conjugate-gradient on the coarstest grid. 

In a first approach we will opt for another strategy. On a coarse grid the data from all individual processes can be distributed over the entire topology by using an All-to-All MPI-call. An exactly identical calculation is then performed on all processes making communication after each iteration step unnecessary. The grid can be also further coarsened down to the level where a direct solution becomes feasible.


\section{Multigrid Cycle}

In the following paragraph we describe a multigrid V-cycle in pseudocode for one iteration of the approximate solution $v_k^m\rightarrow v_k^{m+1}$. For convenience we drop the iteration index $m$.

The multigrid algorithm can be very efficently formulated in terms of a nested procedure. This is because solving $A_k u_k = f_k$ with arbitrary intial guess $v_k$ is equivalent to solving the defect equation $A_{k-1} e_{k-1} = r_{k-1}$ on the next coarser level with $e_{k-1}=0$ as initial guess. This allows mapping onto the original problem with $e_{k-1}$ and $r_{k-1}$ taking now the roles of a solution vector and a right-side vector, respectively. It is thus convenient to re-label the error and residual according to $e_{k-1}\rightarrow v_{k-1}$ and $r_{k-1}\rightarrow f_{k-1}$. This, however, does not affect the meaning of these quantities, in particular that from the first restriction on the solution vector is actually an error and the right-side vector is a residual. 
 
\begin{algorithm}[H]
\caption{Multigrid V-cycle for one iteration step $v_k^m\rightarrow v_k^{m+1}$}
\begin{algorithmic} 
\STATE{}
\STATE{1. Presmoothing:}
\FOR{$\nu_1$ steps}
\STATE{$\triangleright$ Request halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_k v_k = f_k$ on $\partial\Omega_k$ with initial guess $v_k$}
\STATE{$\triangleright$ Send halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_k v_k = f_k$ on $\Omega_k\setminus \partial\Omega_k$  with initial guess $v_k$}
\ENDFOR
\STATE{}
\STATE{2. Compute residual:}
\STATE{$\triangleright$ Compute $r_k=f_k-A_k v_k \rightarrow f_k$}
\STATE{}
\STATE{3. Restrict residual:}
\STATE{$\triangleright$ Compute $f_{k-1}=R^{k-1}_k f_k$}
\STATE{}
\STATE{4. Coarse solve:}
\IF{$k-1>1$}
\FOR{$\nu_1$ steps}
\STATE{$\triangleright$ Request halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_{k-1}v_{k-1}=f_{k-1}$ on $\partial\Omega_{k-1}$ with initial guess $v_{k-1}=0$ }
\STATE{$\triangleright$ Send halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_{k-1}v_{k-1}=f_{k-1}$ on $\Omega_{k-1}\setminus \partial\Omega_{k-1}$ with initial guess $v_{k-1}=0$ }
\ENDFOR
\STATE{$\triangleright$ Set $k-1 \rightarrow k$; goto step 2.}
\ENDIF
\STATE{}
\IF{$k-1=1$}
\STATE{$\triangleright$ Scatter all data via MPI All-to-All call}
\STATE{$\triangleright$ On each process solve $A_1 v_1=f_1$ on entire domain $\Omega_1$, either exactly or using a fast iterative solver with inital guess $v_1=0$} 
\ENDIF
\STATE{}
\STATE{5. Apply coarse grid correction to inital guess:}
\STATE{$\triangleright$ Compute $\hat{v}_k=v_k + I_{k-1}^k v_{k-1}$ on $\Omega_k$}
\STATE{}
\STATE{6. Postsmoothing:}
\FOR{$\nu_2$ steps}
\STATE{$\triangleright$ Request halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_k \hat{v}_k= f_k$ on $\partial\Omega_k \rightarrow v_k$ new approximate solution}
\STATE{$\triangleright$ Send halo data via non-blocking MPI-call}
\STATE{$\triangleright$ Relax $A_k \hat{v}_k= f_k$ on $\Omega_k\setminus\partial\Omega_k \rightarrow v_k$ new approximate solution}
\ENDFOR
\STATE{}
\end{algorithmic}
\end{algorithm}


%\STATE <text>
%\IF{<condition>} \STATE{<text>} \ENDIF
%\FOR{<condition>} \STATE{<text>} \ENDFOR
%\FOR{<condition> \TO <condition> } \STATE{<text>} \ENDFOR
%\FORALL{<condition>} \STATE{<text>} \ENDFOR
%\WHILE{<condition>} \STATE{<text>} \ENDWHILE
%\REPEAT \STATE{<text>} \UNTIL{<condition>}
%\LOOP \STATE{<text>} \ENDLOOP
%\REQUIRE <text>
%\ENSURE <text>
%\RETURN <text>
%\PRINT <text>
%\COMMENT{<text>}
%\AND, \OR, \XOR, \NOT, \TO, \TRUE, \FALSE

\section{Speculation}

The following remarks are relevant for the branch of the code living
in the \texttt{testcharge} directory, where we have SOR (both uniform
permittivity and heterogenous permittivity) and should shortly have a
Fourier solver (uniform permittivity) available.

\subsection{Coordinate system}

The existing code has a single coordinate system which corresponds
to the fine grid $\Omega_h$ in the multigrid picture. Details of the
coordinate system (global and local extent; MPI communicators; 3d-to-1d
indexing functions, etc) are encapsulated in \texttt{coords.c}.
\begin{enumerate}
\item
As a first step towards a multigrid implementation, it would be
desirable to ``objectify'' formally a \texttt{coods\_t} data type.
This would be explicitly referenced in all existing fine grid
contexts (and is desirable on other grounds as well).
\item
For multigrid, one could then instantiate one \texttt{coords\_t}
object for each level of the grid $\Omega_k$ to allow corresponding
grid operations at a given level.
\item
A second option would be to retain a single \texttt{coords\_t} object,
but add functionality to recognise a grid level $k$ and perform the
necessary arithmetic/logic assuming uniform refinement. (This is
probably a less elegant and flexible solution, if more compact.)
\end{enumerate}

\subsection{Grid data structure(s)}

Each level of the grid $\Omega_k$ requires coordinate information,
but also local storage for three-dimensional field quantities, eg.,
$f_k(x,y,z)$. Local field quantities are expected to require a halo
region of extent \texttt{nhalo} cf. existing field quantities on the
fine grid. This allows interpolation and restriction operators to be
evaluated locally.

So, a grid-level structure might contain
\begin{verbatim}
  struct grid_s {
    int k;               /* the grid level */
    coords_t * coords;   /* coordinate information this level */
    field_t * f;         /* rhs */
    ...
  }
\end{verbatim}
A multigrid structure would then contain an array of grids for levels
$k = 1, \ldots, k_{max}$, information replicated on all MPI tasks:
\begin{verbatim}
  struct mgrid_s {
    int kmax;         /* the number of levels */
    grid_t ** level;  /* array of grid objects */
    ...
  }
\end{verbatim}

\subsection{The limit of local cosrsening}

For a given uniform domain decomposition of the fine grid, local
restriction or coarsening can be carried out to the point where
there is a grid of extent one in the local domain. This limits
the extent of local coarsening. At this point, further coarsening
might be based on coarsening the processor grid in the same spirit
as the computational grid.

\subsection{Issues}

\begin{enumerate}
\item
Can we use e.g., PetSc to test some options/performance before starting to
code?
\item
Do we want to limit ourselves to regular system sizes (i.e., 2$^n$)
and processor decompositions?
\item
Can the approach be extended to a threaded model (e.g., GPU)?
\end{enumerate}

\end{document}

