%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  devel.tex
%
%  Information for users and delvelopers
%
%  Edinburgh Soft Matter and Statistical Physics Group and
%  Edinburgh Parallel Computing Centre
%
%  (c) 2014-2017 The University of Edinburgh
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Information for Users and Developers}

This section contains information on the design of the code, along
with details of compilation an testing procedures
which may be of interest to both users and developers.
\textit{Ludwig} is named for Ludwig Boltzmann (1844--1906) to
reflect its background in lattice Boltzmann hydrodynamics.

\subsection{Manifest}

The top level ludwig directory should contain at least the following:
\begin{lstlisting}
$ ls
bash-3.2$ ls
config  LICENSE      mpi_s   src       tests  version.h
docs    Makefile.mk  README  targetDP  util
\end{lstlisting}
The code is released under a standard BSD license; the current version
number is found in \texttt{version.h}. The main source is found in
the \texttt{src} directory, with other relevant code in \texttt{tests}
and \texttt{util}. The \texttt{targetDP} directory holds code related
to the targetDP abstraction layer \cite{gray2013}. These documents are
found in \texttt{docs}.

\subsection{Code Overview}

The code is ANSI C (1999), and can be built without any external
dependencies, with the exception of the message passing interface
(MPI), which is used to provide domain-decomposition based parallelism.
Note that the code will also compile under NVIDIA \texttt{nvcc}, i.e.,
it also meets the C++ standard.

\subsubsection{Design}

The \textit{Ludwig} code has evolved and expanded over a number of
years to its present state. Its original purpose was to investigate
specifically spinodal decomposition in binary fluids (see, e.g.,
\cite{kendon2001}). This work used a free-energy based formulation
combined with a two-distribution approach to the binary fluid problem
(with lattice Boltzmann model D3Q15 at that time). This approach  is
retained today, albeit in a somewhat updated form. The wetting problem
for binary fluid at solid surfaces has also been of consistent interest.
The code has always been developed with
parallel computing in mind, and has been run on a large number of
different parallel machines including the Cray T3E at Edinburgh. These
features, developed by J.-C. Desplat and others,  were reflected in the
early descriptive publication in \textit{Comp.\ Phys.\ Comm} in 2001
\cite{desplat2001}. 

The expansion of the code to include a number of additional features
has occasioned significant alterations over time, and little of the
original code remains. However, the fundamental idea that the code
should essentially operate for high performance computers and be
implemented using ANSI C with message passing via MPI has remained
unchanged.

The code has a number of basic building blocks which are encapsulated
in individual files.

\subsubsection{Parallel environment}

The code is designed around message passing using MPI. A stub MPI
library is provided for platforms where a real MPI is not available
(an increasingly rare occurrence), in which case the code runs in
serial. The parallel environment (interface defined in \texttt{pe.h})
therefore underpins
the entire code and provides basic MPI infrastructure such as
\texttt{info()},
which provides \texttt{printf()} functionality at the root process
only. The parallel environment also provides information on version
number etc. MPI parallelism is implemented via standard domain
decomposition based on the computational lattice (discussed
further in the following section on the coordinate system).

\subsubsection{targetDP: threaded parallelism and vector parallelism}

To allow various threaded models to be included without duplicating
source code, we have developed a lightweight abstraction of the
thread level parallelism, This currently supports a standard single-threaded
model, OpenMP, or CUDA. targetDP (``target data parallel'') allows single
kernels to be written which can then be compiled for the different threaded
models which may be appropriate on different platforms. targetDP can also
be used as a convenient way to express vector parallelism
(typically SSE or AVX depending on processor architecture). targetDP allows
explicit specification of the vector length at compile time.

targetDP does not automatically identify parallelism; this must still be
added appropriately by the developer. It is merely a concise way to include
different threaded models. It is only available in the development version.

\subsubsection{Array address models}

In a related issue, the code allows different data layout to be used
to improve performance. The default layout is array of structures (AOS)
which is suitable for most CPU architectures. (A blocked version AOSOA
is also available which may promote vectorisation.) For GPU machines, a
structure of arrays (SOA) format can be selected at compile time.


\subsubsection{Coordinate system}

It is important to understand the coordinate system used in the
computation. This is fundamentally a regular, 3-dimensional Cartesian
coordinate system . (Even if a D2Q9 LB model is employed, the code is
still fundamentally 3-dimensional, so it is perhaps not as efficient  
for 2-dimensional problems as it might be.)

The coordinate system is centred around the (LB) lattice, with lattice
spacings $\Delta x = \Delta y = \Delta z = 1$. We will refer, in general,
to the lattice spacing as $\Delta x$ throughout, its generalisation to
three dimensions $x,y,z$ being understood. Lattice sites in the
$x-$direction therefore have unit spacing and are located at
$x = 1, 2, \ldots, N_x$.
The length of the system $L_x = N_x$, with the limits of
the computational domain begin $x = 1/2$ and $x = L_x + 1/2$. This allows
us to specify a unit control volume centred on each lattice site $x_i$
being $i-1/2$ to $i + 1/2$. This will become particularly significant for
finite difference (finite volume) approaches discussed later.

Information on the coordinate system, system size and so on is
encapsulated in \texttt{coords.c}, which also deals with the
regular domain decomposition in parallel. Decompositions may be
explicitly requested by the user in the input. or computed by
the code itself at run time. \texttt{coords.h} also provides
basic functionality for message passing within a standard MPI
Cartesian communicator, specification of periodic boundary
conditions, and so on.

Three-dimensional fields are typically stored on the lattice,
but are addressed in compressed one-dimensional format. This
avoids use of multidimensional arrays in C. This addressing
must take account of the width of the halo region required at
the edge of each sub-domain required for exchanging information
in the domain decomposition. The extent of the halo region
varies depending on the application required, and is selected
automatically at run time.

\subsubsection{Lattice Boltzmann hydrodynamics}

The hydrodynamic core of the calculation is supplied by the lattice
Boltzmann method, which was central at the time of first development.
LB is also the basis for hydrodynamic solid-fluid interactions at
stationary walls and for moving spherical colloids. The LB approach
is described in more detail in Section~\ref{section:lb-hydrodynamics}.
For general
and historical references, the interested reader should consider,
e.g., Succi \cite{succi}.

\subsubsection{Free energy}

For complex fluids, hydrodynamics is augmented by the addition of
a free energy for the problem at hand, expressed in terms of an
appropriate order parameter. The order parameter may be a three
dimensional field, vector field, or tensor field. For each problem
type, appropriate tome evolution of the order parameter is supplied.

Coupling between the thermodynamic sector and the hydrodynamics is
abstracted so that the hydrodynamic core does not require alteration
for the different free energies. This is implemented via a series of
call back functions in the free energy which are set appropriately
at run time to correspond to that specified by the user in the
input.

Different (bulk) fluid free energy choices are complemented by
appropriate surface free energy contributions which typically
alter the computation of order parameter gradients at solid
surface. Specific gradient routines for the calculation of
order parameter gradients may be selected or added by the
user or developer.

Currently available free energies are:
\begin{itemize}
\item Symmetric binary fluid with scalar compositional order parameter
$\phi(\mathbf{r})$ and related Cahn-Hilliard equation;
\item Brazovskii smectics, again which scalar compositional order parameter
$\phi(\mathbf{r})$;
\item Polar (active) gels with vector order parameter $P_\alpha (\mathbf{r})$
and related Leslie-Erikson equation;
\item Landau-de Gennes liquid crystal free energy with tensor
orientational order parameter $Q_{\alpha\beta}(\mathbf{r})$, extended to
apolar active fluids and related Beris-Edwards equation;
\item a free energy appropriate for electrokinetics for charged fluids
and related Nernst-Planck equations (also requiring the solution of the
Poisson equation for the potential);
\item a coupled electrokinetic binary fluid model;
\item a liquid crystal emulsion free energy which couples a binary
composition to the liquid crystal.
\end{itemize}
See the relevant sections on each free energy for further details.

\subsection{Compilation}

Compilation of the main code is controlled by the \texttt{config.mk} in
the  top-level directory. A number of example \texttt{config.mk} files
are provided in the \texttt{config} directory (which can be copied and
adjusted as needed). This section discusses a number of issues
which influence compilation.

\subsubsection{Dependencies on third-party software:}
There is the option to use PETSC to solve the Poisson equation required in
the electrokinetic problem. A rather less efficient in-built method
can be used if PETSC is not available.
We suggest using PETSC
v3.4 or later available from Argonne National Laboratory
\texttt{http://www.mcs.anl.gov/petsc/}.

\vfill
\pagebreak

