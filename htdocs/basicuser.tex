%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  user.tex
%
%  The 'user' section which holds an overview, and discusses
%  compilation and running.
%
%  $Id: user.tex 1694 2012-06-28 09:44:12Z stratford $
%
%  Edinburgh Soft Matter and Statistical Physics Group and
%  Edinburgh Parallel Computing Centre
%
%  Kevin Stratford (kevin@epcc.ed.ac.uk)
%  (c) 2010 The University of Edinburgh
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic User Input}

\subsection{Parallel Performance}

\subsubsection{General Comments}

The basic LB calculation should scale well in parallel, that is,
the time taken for fixed problem size will decrease linearly
with the number of MPI processes (strong scaling), or a larger
problem can be run on a proportionally larger number processes
in the same time (weak scaling). To ensure performance is retained
in parallel, it is useful to understand some basic considerations.
Some of these are discussed below.

\textit{Ludwig} has been run successfully on up to 131,072 MPI
tasks with close to ideal scaling, and there is no problem in
principle to run larger decompositions.

\subsubsection{Things to Remember and Things to Avoid}
\label{section-advice}

There are a number of simple issues to get right:
\begin{itemize}
\item
Make sure the assertions are switched off (via preprocessor option
\texttt{-DNDEBUG}) and appropriate optimisations
for the current compiler are switched on for production runs (see Makefile).
\item
Do not use too small a local domain size. Typically, 16$^3$ to 32$^3$
cubic local domains will give resaonble scaling. Anything smaller may
be inefficient. The best size may depend on the exact nature of the
calculation and the hardware. Note that this limitation will always
ultimately limit strong scaling.
\item
Don't have diagnsotic output (freq\_statistics) more often than is
necessary. This output requires a global communication which is slow.
\item
Use BINARY output options; ASCII output is slower, and produces larger
files.
\item
Do not use serial I/O. This will have an increasingly high overhead as
the number of processors is increases (it's probably reasonable to
128--256 MPI tasks). This is because each MPI tasks writes its data,
in turn, to a single file; this in inherently serial. To avoid this,
you need to specify the I/O decomposition for the various output
quantities of interest. This splits the system into regular blocks
(in a similar approach to the MPI decomposition), the data for which
are written to separate files in parallel. For example, if the MPI
decomposition is 8\_8\_8 (512 MPI tasks), then a reasonable I/O
decomposition may be 2\_2\_2 or 4\_4\_4. The only way to find out
what works best is to try some tests for the problem size you want to
address.
\item
Don't use the Ewald sum! This simply does not scale.
An alternative algorithm is required.
\end{itemize}

\subsection{Setting up the input}

The executable will look for an input file at run time.
By default, this file is named \texttt{input} and should be in the
current working directory. The alternative is to specify the name
of the file explicitly on the command line:
\begin{verbatim}
$ ./Ludwig.exe input_file
\end{verbatim}

A reference input file \texttt{input.ref} is provided as a template.

The contents of the input file are made up of \textit{key value}
pairs with control the run-time behaviour. Blank lines and lines
begining with \# are ignored as comments. The following describes
the effect of various keys. Most keys have a default value which
will be used by the code if the corresponding key is not present
(or commented out) in the input file.


Decide what free energy you want and check the relevant sections
of this dcoumentation. If you want a simple fluid only, the free
energy should be set to \texttt{none}.

You should set at least the basic fluid parameters, the system
size, and number of steps to be run.

\textbf{IMPORTANT:} Although many incompatible choices of input
parameters are trapped at run time, you must be careful in how
the input file is set out. If you mistakenly set or unset some
options in the input, erroneous results will surely result...


\subsection{Basic run parameters}

\inputkey{N\_cycles}

The number of lattice Boltzmann time steps to execute. Default
value: 0.

\inputkey{N\_start}

By default the code will start from time step $t = 0$. If you 
wish to restart from a previously saved configuration, set
the appropriate value for \texttt{N\_start}. The code will
execute \texttt{N\_cycles} steps starting from this point.

\inputkey{size}

Controls the number of lattice points in the $(x, y, z)$ directions,
respectively. Default value: 64\_64\_64. If a two-dimensional system
is required, the $z$-direction can be set to 1.

\inputkey{grid}

In parallel, this sets the processor Cartesian decomposition, i.e.,
the number of processors in each dimension of the Cartesian communicator.
The default value may be implementation dependent as it is that returned
by \texttt{MPI\_Dims\_create()}. For
example:
\begin{verbatim}
size 64_64_64
grid 4_2_1
\end{verbatim}
gives a total (physical) system size of 64 lattice sites in each
direction, decomposed across 4 processors in the x-direction, 2 in
the y-direction and 1 (no decomposition) in the z-direction. The
total number of processors must therefore be 8. The local domain
size per processor is then 16x32x64.


\subsection{Fluid properties}

\inputkey{free\_energy}

This sets the free energy in use, and hence has many consequences.
The default value is \texttt{none}, i.e., use a simple fluid only.
The available choices are for symmetric binary fluids, Brazovskii
smectics, polar active gels, and liquid crystals. The surfactant
model is currently being rebuilt.

\inputkey{viscosity}

Sets the LB fluid shear viscosity $\eta$ (and related relaxation time).
Safe values are roughly $0.2 > \eta > 0.0001 $. Default value is 1/6
(i.e., relaxation time equal to unity).

\inputkey{viscosity\_bulk}

Sets the LB bulk viscosity $\zeta$ (and related relaxation time).
This can be useful if you are concerned with incompressiblity
violations, in which case $\zeta >> \eta$ can be useful. The
default value is $\zeta = \eta$.

\inputkey{isothermal\_fluctuations}

This switches on the fluctuationing hydrodynamics. Default is off.

\inputkey{temperature}

The LB 'temperature' using fluctuating hydrodynamics. Safe values
are $0.0001 > kT > 0$. 

\inputkey{ghost\_modes}

Allows you to switch the ghost modes off in the collision stage.
Default is on.


\subsection{Boundary conditions}

Solid planar boundary walls can be added at the extremities of the
system (i.e., the volume of fluid is exactly that specified by
the \texttt{size} keyword). The wall can be in one, two or all three
coordinate directions, i.e., a 'slab' geometry, and 'duct' geometry
or a 'box' geometry, respectively.

The boundary walls are switched on via the keyword

\inputkey{boundary\_walls}

and with a complementatary specification of the key \texttt{periodicity}.

For example, walls at $x = x_{\min}$  and $x = x_{\max}$  are specified
by

\inputkey{boundary\_walls 1\_0\_0}

\inputkey{periodicity 0\_1\_1}

Both must be present. Note that there are a number of constraints on
valid choices, and the code will stop if there is an invalid choice, or
the two keys above do not match. Common choices are:
\texttt{boundary\_walls 0\_0\_1},
which gives periodic boundaries in the $x-$ and $y-$directions, with
walls at either edge in the $z-$direction; \texttt{0\_1\_1}
gives periodic conditions in the $x-$ direction only, while
\texttt{1\_1\_1} is a fully enclosed box.

In the case of the slab geometry with walls at $z = z_{\min}$ and
$z= z_{\max}$ (only),
two further values are used so that shear may be imparted to the system:

\inputkey{boundary\_speed\_bottom}

\inputkey{boundary\_speed\_top}

These values set the $x-$component of the wall velocity at
$z = z_{\min}$ and $z = z_{\max}$, respectively. Like all
velocities, these values must respect the low Mach number
constraint $u_x < c_s$.

There is a further option for use with the special case in which
top and bottom walls in the $z-direction$ are used to drive a shear
flow (typically with the top speed positive and the bottom speed
negative). The key

\inputkey{boundary\_shear\_init}

may be switched on (set to 1) to provide initialisation of the
LB distributions appropriate for the specified shear rate based
on a linear velocity profile in the $z-$direction. The default
is to initialise the fluid at rest.

\subsubsection{Lees-Edwards planes}

\inputkey{N\_LE\_plane}

sets the total number of Lees-Edwards planes. Default is zero.
The placing is as followings. The number of planes $n$ must
divide the lattice size in the $x$-direction to give an integer
$\delta x$. Planes are then placed at $\delta x / 2, 3\delta x/2, \ldots$.

\inputkey{LE\_plane\_vel}

sets the velocity of each plane relative to the lattice. All planes
have the same, constant, velocity.

\inputkey{LE\_init\_profile}

if set to 1, the fluid velocity is initialise to reflect a steady
state shear flow appropriate for the number of planes at the
given velocity. If set to zero, the fluid is initialised with
zero velocity.

Note that when the Lees Edwards boundaries are in place, output
files containing lattice quantities must be `unrolled' to remove
the time-dependent displacement of the planes. This is done using
the extraction utility (see section on parallel I/O).

The code works out the
current displacement of the planes by computing $U_{LE} t$, where
$t$ is the current time step. A shear run should then start from
$t = 0$, otherwise an offset is required.

\inputkey{LE\_time\_offset}

It is often convenient to run an equilibration with no shear, and
then to start an experiment after some number of steps. This
key allows you to offset the start of the Lees-Edwards motion.
It should then take the value of the start time (in time steps)
corresponding to the restart at the end of the equilibration
period. You also need to take this value into account when unrolling
the output.

\subsubsection{Lees-Edwards planes in parallel} 

There are a couple of additional constraints to use the Lees-Edwards
planes in parallel. In particular, the planes cannot fall at a
processor boundary in the $x$-direction. This means you should
arrange an integer number of planes per process in the $x$-direction.
(For example, use one plane per process; this will also ensure the number
of planes
still evenly divides the total system size.)
This will interleave the planes with the processor decomposition.
The $y$-direction and $z$-direction may be decomposed without
further constraint.

Note that this means a simulation with one plane will only work
if there is one process in the $x$ decomposition.

\subsection{Input/Output}

\subsubsection{Serial I/O and standard output}

A variety of information is printed to standard output during the
course of the simulation. This should allow the user to keep track
of basic progress in terms of integrated quantities, total mass,
total momentum and so on. Frequency of output is controlled by

\inputkey{freq\_statistics}

which is an integer. Note that these global statistics require
global communication, and so can affect performance in parallel.
It is recommended that the frequency is kept to a minimum for
large systems (e.g., every 500 or 1000 time steps). This
consideration is true for most types of output (see section
on parallel I/O).

\subsubsection{Configurations}

It may be appropriate to save entire model configurations from
time to time, and certainly when a restart is wanted. For each lattice
quantity, in addition to the data themselves, the program produces a
single meta-data file (ending in
\texttt{.meta}) which is used to describe these data. Colloid
data does not require a meta-data file at the moment.

\inputkey{freq\_config}

This is an integer and sets the frequency of full configuration dumps.
These may be large depending on the size of the system.
Full configurations should consist of the LB distributions,
order parameter (if present) and colloids (if present).


\inputkey{config\_at\_end [yes | no]} 

forces (or switches off) a configuration dump at the end of the run.
The default is to produce a configuration at the end of the run.

\subsubsection{Other lattice quantities}

A number of other lattice-based quantities are available as output.

\inputkey{freq\_measure}

sets the frequency of recording of colloid/order parameter output.
This includes the combined scalar order parameter and director
field output in the case of the liquid crystal free energy.

\inputkey{freq\_phi}

set frequency of order parameter output.

\inputkey{freq\_vel}

set frequency of velocity output. Note that all the hydrodynamic
quantities can be reconstructed from the distribution output, if
available. If only the velocity field is required, this method
allows a considerable saving in storage.

\subsubsection{Colloid I/O}

Colloid I/O is controlled via a series of key value pairs. These are:

\inputkey{colloid\_io\_freq}

gives output at this frequency in time steps. Output is also generated
at configuration steps (and measurement steps at the moment).

\inputkey{colloid\_io\_grid} integer vector

sets the colloid I/O grid. It must be accomodated by the current
choice of \texttt{grid}.

\inputkey{colloid\_io\_format\_input}

One of \texttt{ASCII}, \texttt{ASCII\_SERIAL}, \texttt{BINARY}, or
\texttt{BINARY\_SERIAL}

\inputkey{colloid\_io\_format\_output}

For output, only \texttt{ASCII} or \texttt{BINARY} are available.
 

\subsubsection{I/O format}


\inputkey{vel\_format}

\inputkey{phi\_format}

May be either \texttt{ASCII} or \texttt{BINARY}. Default is \texttt{BINARY}.
Configuration I/O is always in binary. It is recommended to use
binary output for speed and disk space considerations. For a local
sub-domain, the order of the output in the file follows the standard
loop order in the code, i.e., with the $z$-direction running fastest,
then $y$, and with $x$ running slowest. For vector quantities
(anything with more components than a scalar), the vector at each
lattice site appears contiguously.

\subsubsection{Parallel I/O}

In serial, each output quantity appears in one file (\texttt{phi-}
and so on). In parallel, this is also the case by default. However,
this does not scale, owing to the fact that each process has to
write to the same file in turn (ie., in serial). The solution is
to make the I/O parallel. This is done by splitting the domain into
different ``I/O grids'' each of which writes to a separate file.

The default I/O grid can be set using, e.g.,

\inputkey{default\_io\_grid 2\_2\_2}

meaning that the domain is decomposed into a 2x2x2 Cartesian
decomposition for I/O purposes. This will give rise to a total
of 8 files per I/O event with extensions \texttt{.008-001} to
\texttt{.008-008}. These files must be recombined if analysis
is required. However, restarting can be performed with the
parallel files.

Individual lattice quantities can use separate I/O grids. This
may be particularly useful for the distribution data, which are
much larger than the other lattice quantities. This means a
finer decomposition may be appropriate. Clearly, the size of the
I/O decomposition cannot exceed the processor decomposition.
See the advice in section \ref{section-advice} for guidelines.

\subsection{Miscellaneous}

\inputkey{random\_seed}

sets the random number generator seed.

\subsection{Dealing with the Parallel I/O files}

When running in parallel, output of lattice-based (and particle)
quantities takes place in parallel. As a consequence, the order
of the output in the files is not in the same order as would be
the case in serial. This means the parallel output must be
manipulated into the correct order before any analysis.

Parallel I/O subdivides the processors into one or more groups,
each of which write data to a separate file. Information on the
content of each of these files is contained in `meta-data' file
for each quantity, which are produced automatically at the start
of each run. The files are called, e.g., \texttt{phi.001-001.meta}.
If there are 8 I/O groups, the files will be named \texttt{008-001}
to \texttt{008-008} for the different groups.

The actual data is stored for each relevant time step as, e.g.,
\texttt{phi-000100.001-001} where the extension refers to the
I/O. To recombined a single, or multiple, data files into a
single file in the correct order, a utility program
\texttt{extract.c} is provided.

The executable takes two arguments, e.g.,

\inputkey{./extract phi.001-001.meta phi-000100.001-001}

where  the first is the first meta-data file in the relevant series,
and the second is the first data file for the relevant time step. The
program will combine the relevant quantities in the correct order
and produce a single new file \texttt{phi-000100} (in the above case).

\subsection{Restarting}

When restarting in parallel, the processor decomposition must be
preserved. An attempt to change the processor decomposition will
cause the configuration files to be read in the incorrect order,
and results will be wrong.


