%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  develop.tex
%
%  Further information for delvelopers
%
%  Edinburgh Soft Matter and Statistical Physics Group and
%  Edinburgh Parallel Computing Centre
%
%  (c) 2014-2017 The University of Edinburgh
%
%  Contributing authors:
%  Kevin Stratford (kevin@epcc.ed.ac.uk)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Further Information for Developers}

\subsection{Address models for 3-dimensional fields}

The code allows for different order of storage associated with 3-dimensional
fields (scalars, vectors, and so on). These storage schemes are usually
referred to as array of structures (AOS), structure of arrays (SOA), and
for the blocked version array of structures of (short) arrays (AOSOA).
For historical reasons these are
also sometimes referred to as `Model' and `Model~R' options, which correspond
to array-of-structures and structure-of-arrays layout, respectively. Early
versions of the code for CPU were written to favour summation of LB
distributions on a per lattice site basis in operations such as
$\rho(\mathbf{r}) = \sum_i f_i(\mathbf{r})$. This is array-of-structures,
where the $f_i$ are stored contiguously per site. Introduction of GPU
versions, where memory coalescing favours the opposite memory order,
were then referred to as `Model~R', the `R' standing for `reverse'.

The memory layouts are discussed below. In all cases, a 3-d field occupies
lattice sites with a unique spatial index determine by position, and
computed via \texttt{coords\_index()}. These position indices will be denoted
$r_0, r_1, \ldots, r_{n_s-1}$ where $n_s$ is the total number of lattice
sites (including halo points).

\subsubsection{Rank 1 objects (to include scalar fields)}
\label{subsection:addressing-model-rank1}

\textbf{ADDR\_AOS}: The array-of-structures order for an
$n-$vector field
with components $ v_\alpha = (v_0, v_1, \ldots, v_{n-1})$ is, schematically:
\[
\underbrace{ \boxed{v_0\vphantom{v_{n-1}}} \boxed{v_1\vphantom{v_{n-1}}}
\boxed{\ldots\vphantom{v_{n-1}}} \boxed{v_{n-1}} }_{r_0}
\underbrace{ \boxed{v_0\vphantom{v_{n-1}}} \boxed{v_1\vphantom{v_{n-1}}}
\boxed{\ldots\vphantom{v_{n-1}}} \boxed{v_{n-1}} }_{r_1}
\underbrace{ \boxed{v_0\vphantom{v_{n-1}}} \boxed{v_1\vphantom{v_{n-1}}}
\boxed{\ldots\vphantom{v_{n-1}}} \boxed{v_{n-1}} }_{r_2} \ldots
\underbrace{ \boxed{v_0\vphantom{v_{n-1}}} \boxed{v_1\vphantom{v_{n-1}}}
\boxed{\ldots\vphantom{v_{n-1}}} \boxed{v_{n-1}} }_{r_{n_s-1}}
\]

\textbf{ADDR\_SOA}: The structure-of-arrays version is:
\[
\overbrace{
\begin{array}{cc} \boxed{v_0} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_0} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_0}} \\ \phantom{r_2}\end{array} \mkern-22mu
\begin{array}{cc} \boxed{v_0} \\  _{r_{n_s-1}} \end{array}
}^{component_0}
\overbrace{
\begin{array}{cc} \boxed{v_1} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_1} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_0}} \\ \phantom{r_2}\end{array} \mkern-22mu
\begin{array}{cc} \boxed{v_1} \\  _{r_{n_s-1}} \end{array}
}^{component_1}
\begin{array}{cc} \ldots \\ \phantom{r_2} \end{array}
\overbrace{
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_{n-1}}} \\ \phantom{r_2}\end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_{n_s-1}} \end{array}
}^{component_{n-1}}
\]

A scalar field has $n=1$.

% ADDR_MACRO(n_s, n, i, \alpha) in this notation corresponds to
% ADDR_MACRO(nsites, nfield, index, ifld)

\subsubsection{Rank 2 objects (to include dyadic tensor fields)}

\textbf{ADDR\_AOS}:
A general rank 2 tensor $t_{\alpha\beta}$ with components $(t_{0,0},
\ldots, t_{m-1,n-1})$ is stored as:
\begin{gather*}
\underbrace{
\boxed{t_{0,0}\vphantom{t_{m-1,n-1}}} \boxed{t_{1,0}\vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,0}\vphantom{t_{m-1,n-1}}}
\boxed{t_{0,1}\vphantom{t_{m-1,n-1}}} \boxed{t_{1,1}\vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,1}\vphantom{t_{m-1,n-1}}}
\ldots
\boxed{t_{0,n-1}\vphantom{t_{m-1,n-1}}}\boxed{t_{1,n-1} \vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,n-1}\vphantom{t_{m-1,n-1}}}
}_{r_0}
\ldots
\\
\ldots
\underbrace{
\boxed{t_{0,0}\vphantom{t_{m-1,n-1}}} \boxed{t_{1,0}\vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,0}\vphantom{t_{m-1,n-1}}}
\boxed{t_{0,1}\vphantom{t_{m-1,n-1}}} \boxed{t_{1,1}\vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,1}\vphantom{t_{m-1,n-1}}}
\ldots
\boxed{t_{0,n-1}\vphantom{t_{m-1,n-1}}}\boxed{t_{1,n-1} \vphantom{t_{m-1,n-1}}}
\boxed{\ldots\vphantom{t_{m-1,n-1}}} \boxed{t_{m-1,n-1}\vphantom{t_{m-1,n-1}}}
}_{r_{n_s-1}}
\end{gather*}
Dyadic tensors, for example the gradient of a vector field
$\partial_\alpha v_\beta$ in three dimensions, are stored in corresponding
fashion with $m=3$ and
$\partial_\alpha = (\partial_x, \partial_y, \partial_z)$.

\textbf{ADDR\_SOA}: The structure-of-arrays version is
\[
\overbrace{
\begin{array}{cc} \boxed{t_{0,0}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{t_{0,0}} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{t_{0,0}}} \\ \phantom{r_2}\end{array} \mkern-18mu
\begin{array}{cc} \boxed{t_{0,0}} \\  _{r_{n_s-1}} \end{array}
}^{component_{0,0}}
\overbrace{
\begin{array}{cc} \boxed{t_{1,0}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{t_{1,0}} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{t_{0,0}}} \\ \phantom{r_2}\end{array} \mkern-18mu
\begin{array}{cc} \boxed{t_{1,0}} \\  _{r_{n_s-1}} \end{array}
}^{component_{1,0}}
\begin{array}{cc} \ldots\vphantom{t_{n-1}} \\ \phantom{r_2}\end{array}
\overbrace{
\begin{array}{cc} \boxed{t_{m-1,n-1}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{t_{m-1,n-1}} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{t_{0,0}}} \\ \phantom{r_2}\end{array} \mkern-17mu
\begin{array}{cc} \boxed{t_{m-1,n-1}} \\  _{r_{n_s-1}} \end{array}
}^{component_{m-1,n-1}}
\]

% ADDRRES_MACRO(n_s, m, n, i, alpha, beta) corresponds to, e.g.,
% ADDRESS_MACRO(nsites, NVECTOR, NQAB, index, ia, ib)

\subsubsection{Compressed rank 2 objects}

A symmetric tensor $S_{\alpha\beta}$ in three dimensions has six independent
components. It may be convenient to store this in compressed form as a
rank 1 vector $(S_{xx}, S_{xy}, S_{xz}, S_{yy}, S_{yz}, S_{zz})$ to eliminate
redundent storage.

A symmetric traceless rank 2 tensor --- for example, the Landau-de Gennes
liquid crystal order parameter $Q_{\alpha\beta}$ --- has five independent
components. This is stored as a rank 1 vector with five components
$(Q_{xx}, Q_{xy}, Q_{xz}, Q_{yy}, Q_{yz})$ to eliminate redundant
storage. API calls are provided to expand the compressed format to the
full rank-2 representation $Q_{\alpha\beta}$ and, conversely, to compress
the full representation to five components.

\subsubsection{Rank 3 objects (to include triadic tensor fields)}

The general rank 3 object $t_{\alpha\beta\gamma}$ with components
$(t_{0,0,0}, \ldots, t_{m-1,n-1,p-1})$ is stored in a manner which
generalises from the above, i.e., with the rightmost index running
fastest. Diagrams are omitted, but AOS and SOA storage patterns
follow the same form as seen above.

A triadic tensor, for example the general second derivative of a vector
field $\partial_\alpha \partial_\beta v_\gamma$ may be stored as a rank
3 object.

\subsubsection{Compressed rank 3 objects}

Symmetry properties may be used to reduce the storage requirement
associated with rank 3 tensors. For example, the gradient of the
liquid crystal order parameter $\partial_\gamma Q_{\alpha\beta}$
may be stored as a rank 2 object. The exact requirement will depend
on the construction of the tensor.

%
% The generalised macro would be
% ADDR_MACRO(nsites, m, n, p, i, alpha, beta, gamma)

\subsubsection{LB distribution data}

\textbf{ADDR\_AOS}: For the LB distributions, up to two distinct
distributions can be accommodated to allow for a binary fluid
implementation, although the usual situation is to have only one.
The AOS order for two $N$-velocity distributions $f$ and $g$ is:
\[
\underbrace{
\boxed{f_0\vphantom{f_{N-1}}}    \boxed{f_1\vphantom{f_{N-1}}}
\boxed{\ldots\vphantom{f_{N-1}}} \boxed{f_{N-1}} \,
\boxed{g_0\vphantom{f_{N-1}}}        \boxed{g_1\vphantom{f_{N-1}}}
\boxed{\ldots\vphantom{f_{N-1}}} \boxed{g_{N-1}\vphantom{f_{N-1}}}}_{r_0}
\ldots
\underbrace{
\boxed{f_0\vphantom{f_{N-1}}}    \boxed{f_1\vphantom{f_{N-1}}}
\boxed{\ldots\vphantom{f_{N-1}}} \boxed{f_{N-1}} \,
\boxed{g_0\vphantom{f_{N-1}}}        \boxed{g_1\vphantom{f_{N-1}}}
\boxed{\ldots\vphantom{f_{N-1}}} \boxed{g_{N-1}\vphantom{f_{N-1}}}}_{r_{n_s=1}}
\]

\textbf{ADDR\_SOA}: The strucuture-of-arrays order is:

\[
\overbrace{
\overbrace{
\begin{array}{cc} \boxed{f_0} \\  _{r_0} \end{array} \mkern-18mu
\begin{array}{cc} \boxed{\ldots\vphantom{f_0}} \\ \phantom{r_2}\end{array} \mkern-22mu
\begin{array}{cc} \boxed{f_0} \\  _{r_{n_s-1}} \end{array}
}^{f_0}
\begin{array}{cc} \ldots \\ \phantom{r_2} \end{array}
\overbrace{
\begin{array}{cc} \boxed{f_{N-1}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{f_{N-1}}} \\ \phantom{r_2}\end{array} \mkern-17mu
\begin{array}{cc} \boxed{f_{N-1}} \\  _{r_{n_s-1}} \end{array}
}^{f_{N-1}}
}^{f_i}
\overbrace{
\overbrace{
\begin{array}{cc} \boxed{g_0\vphantom{f_{N-1}}} \\ _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{f_{N-1}}} \\ \phantom{r_2}\end{array} \mkern-22mu
\begin{array}{cc} \boxed{g_0\vphantom{f_{N-1}}} \\  _{r_{n_s-1}} \end{array}
}^{g_0} \ldots
\overbrace{
\begin{array}{cc} \boxed{g_{N-1}\vphantom{f_{N-1}}} \\ _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{f_{N-1}}} \\ \phantom{r_2}\end{array} \mkern-17mu
\begin{array}{cc} \boxed{g_{N-1}\vphantom{f_{N-1}}} \\  _{r_{n_s-1}} \end{array}
}^{g_{N-1}\vphantom{f_{N-1}}}
}^{g_i} 
\]
For the single-distribution case, this is equivalent to the rank 1 vector
field with $n=N$.

\subsection{Generalised model for SIMD vectorisation}

To promote parallelism at the instruction level, it is necessary to
insure the innermost loop of any loop construct has an extent which
is the vector length for the target architecture. The memory layout
should therefore be adjusted accordingly. This means the MODEL format
is generalised to a (rather clumsily named) array of structures of
short vectors. The length of the short vectors is the SIMD vector
length.

The outermost loop in this context is always to be the loop over
lattice sites, which is adjusted according to the vector length;
see Section \ref{subsection:addressing-how-to} below for practical
examples.

The SOA picture, which targets coalescence, can be viewed as
naturally supporting SIMD vectorisation by virtue of allowing
contiguous access to individual quantities as a function of lattice
index. (If SIMD vectorisation is wanted at all on GPU architecture,
it can be as a means to relieve register pressure.) SOA therefore
remains unaltered and we concentrate on the AOS picture.

\subsubsection{Rank 1 objects}

\textbf{ADDR\_AOSOA}: The structure of short vectors is based on the
SIMD vector length which we here denote $V$:
\[
\underbrace{
\overbrace{
\begin{array}{cc} \boxed{v_0} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_0} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_0}} \\ \phantom{r_2}\end{array} \mkern-20mu
\begin{array}{cc} \boxed{v_0} \\  _{r_{V-1}} \end{array}
}^{component_0}
\overbrace{
\begin{array}{cc} \boxed{v_1} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_1} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_0}} \\ \phantom{r_2}\end{array} \mkern-20mu
\begin{array}{cc} \boxed{v_1} \\  _{r_{V-1}} \end{array}
}^{component_1}
%\begin{array}{cc} \ldots \\ \phantom{r_2} \end{array}
\overbrace{
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_0} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_1} \end{array} \mkern-17mu
\begin{array}{cc} \boxed{\ldots\vphantom{v_{n-1}}} \\ \phantom{r_2}\end{array}
\mkern-17mu
\begin{array}{cc} \boxed{v_{n-1}} \\  _{r_{V-1}} \end{array}
}^{component_{n-1}}}_{\mathrm{SIMD\ BLOCK}}
\quad\ldots
\]
Subsequent SIMD blocks involve lattice sites $r_V \ldots r_{2V-1}$,
$r_{2V} \ldots r_{3V-1}$, and so on. If the SIMD vector length is unity,
this collapses to the standard Model picture shown in
Section~\ref{subsection:addressing-model-rank1}.

The generalisation of this approach to rank 2 and rank 3 objects 
follows the corresponding Model implementation.

\subsection{Addressing: How to?}
\label{subsection:addressing-how-to}

\subsubsection{Compilation}

A number of options exist to control at compilation time the choice
of the address model. If no options are specified the default
addressing model is AOS. Two additional C preprocessor directives
exist:

\texttt{-DADDR\_SOA} selects structure-of-arrays order (e.g., for
coallescing on GPU platforms).

\texttt{-DADDR\_AOSOA} block SOA pattern (usually combined with
a non-unit vector length).

For development purposes, the address functions allow the array indices
to be checked against the correct range. For production runs, the
relevant functions are replaced by macros if \texttt{-DNDEBUG} is
specified.

The targetDP vector length is specified via, e.g.,  \texttt{-DVVL=4}.

\subsubsection{Kernels}

To provide a transparent interface for addressing vector fields,
a single API is provided which implements the addressing order
selected at compile time. This interface is always based
on a fixed order of indices which may include the
vector index of the innermost SIMD loop if present. This
allows both vectorised and non-vectorised loops to be constructed
in a consistent manner.

The usual model for constructing a loop involving all lattice sites
(in this case without haloes) would be, in the absence of vectorisation:
\begin{lstlisting}
for (ic = 1; ic <= nlocal[X]; ic++) {
  for (jc = 1; jc <= nlocal[Y]; jc++) {
    for (kc = 1; kc <= nlocal[Z]; kc++) {

      index = coords_index(ic, jc, kc);
      /* Perform operation per lattice site ... */
    }
  }
}
\end{lstlisting}
where final array indexing is based on the coordinate index and is
specific to the object at hand. To allow transparent switching
between different addressing models, the indexing must be abstracted.
The abstraction is provided in \texttt{memory.h}, where functions
(or function-like macros) are provided to make the appropriate
address computation.
As a concrete example, the following considers a rank 1 3-vector:
\begin{lstlisting}
for (ic = 1; ic <= nlocal[X]; ic++) {
  for (jc = 1; jc <= nlocal[Y]; jc++) {
    for (kc = 1; kc <= nlocal[Z]; kc++) {

      index = coords_index(ic, jc, kc);
      for (ia = 0; ia < 3; ia++) {
        iref = addr_rank1(nsites, 3, index, ia);
        array[iref] = ...;
      }
    }
  }
}
\end{lstlisting}
Here, the function \texttt{addr\_rank1()} performs the appropriate
arithmetic to reference the correct 1-d array element in either
address model. We note that
this can be implemented to operate appropriately when the SIMD vector
length is an arbitrary number.

This will usually be carried out in computational kernels, where the
\texttt{kernel.h} interface can be used to help to manage the
one-dimensional loop structure required the the targetDP threading
model. In this way, the kernel launch stage will specify the limits
of the kernel in terms of local $x$-coordinates, $y$-coordinates, and
$z$-coordinates in the usual way. The kernel itself is then written
in terms of a one-dimensions kernel index.

In the following schematic, the launch stage defines a kernel context
to be passed to the kernel along with the other actual arguments:
\begin{lstlisting}
  kernel_info_t limits;
  kernel_ctxt_t * ctxt;

  limits.xmin = 1; limits.xmax = nlocal[X];
  limits.ymin = 1; limits.ymax = nlocal[Y];
  limits.zmin = 1; limits,zmax = nlocal[Z];

  kernel_ctxt_create(NSIMDVL, limits, &ctxt);

  __host_launch(kernel_function, nblk, ntpb, ktx, ...);
\end{lstlisting}


An suitable  vectorised kernel may be constructed as follows with the
appropriate base index for the loop computed via the utility
\texttt{kernel\_baseindex()}:
\begin{lstlisting}
for (kindex = 0; kindex < kiterations; kindex += SIMDVL) {
  index = kernel_baseindex(ctxt, kindex);
  for (ia = 0; ia < 3; ia++) {
    for (iv = 0; iv < SIMDVL; iv++) {
      iref = addr_rank1(nsites, 3, index + iv, ia);
      array[iref] = ...;
    }
  }
}
\end{lstlisting}
Note that the same function \texttt{addr\_rank1()} with the 
argument which is the index of the vector loop is used. Note also that
vectorised versions must take into account the extent that the kernel
extends into the halo region, which involves
logic to avoid lattice sites which are not required (this type of
masking operation is not shown in this
example).

\vfill
\pagebreak

